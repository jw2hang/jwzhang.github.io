---
title: "On the convergence analysis of muon"
date: 2025-05-29
publishDate: 2025-05-29
authors: ["Wei Shen", "Ruichuan Huang", "Minhui Huang", "Cong Shen", "Jiawei Zhang"]
publication_types: ["3"]
abstract: "The majority of parameters in neural networks are naturally represented as matrices. However, most commonly used optimizers treat these matrix parameters as flattened vectors during optimization, potentially overlooking their inherent structural properties. Recently, an optimizer called Muon has been proposed, specifically designed to optimize matrix-structured parameters. Extensive empirical evidence shows that Muon can significantly outperform traditional optimizers when training neural networks. Nonetheless, the theoretical understanding of Muon's convergence behavior and the reasons behind its superior performance remain limited. In this work, we present a comprehensive convergence rate analysis of Muon and its comparison with Gradient Descent (GD). We further characterize the conditions under which Muon can outperform GD. Our theoretical results reveal that Muon can benefit from the low-rank and approximate blockwise diagonal structure of Hessian matrices -- phenomena widely observed in practical neural network training. Our experimental results support and corroborate the theoretical findings."
featured: false
publication: "*arXiv preprint arXiv:2505.23737*"
---
